{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"graphcodebert-20sample-es-cos-awp-cv.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyN0HbF5eJHc6VOlRyxEtZHr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gL_kkFS6chBn","executionInfo":{"status":"ok","timestamp":1658985130823,"user_tz":-540,"elapsed":7628,"user":{"displayName":"조문근","userId":"14792891422128059653"}},"outputId":"a2a0f1cd-e564-41b9-cfbc-2a45ff773988"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Thu Jul 28 05:12:09 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","!pip install -q transformers\n","\n","!nvidia-smi"]},{"cell_type":"markdown","source":["# EarlyStopping"],"metadata":{"id":"-1qNgXrfc1xi"}},{"cell_type":"code","source":["import copy\n","import torch\n","import numpy as np\n","\n","class EarlyStopping:\n","    def __init__(self, patience=6, mode=\"max\", max_epoch=1e6, min_epoch=0, at_last_score=None):\n","        self.patience = patience\n","        self.mode = mode\n","        self.max_epoch = max_epoch\n","        self.min_epoch = min_epoch\n","        self.at_last_score = at_last_score if at_last_score is not None else -np.Inf \n","        self.epoch = 0\n","        self.early_stop = False\n","        self.best_model = None\n","        self.best_epoch = 0\n","        self.model_path = None\n","        self.best_score = -np.Inf if self.mode == \"max\" else np.Inf\n","\n","    def __call__(self, epoch_score, model=None, model_path=None):\n","        self.model_path = model_path\n","        self.epoch += 1\n","\n","        score = -epoch_score if self.mode == \"min\" else epoch_score\n","        \n","        if score <= self.best_score: \n","            counter = self.epoch - self.best_epoch\n","            print('EarlyStopping counter: {} out of {}'.format(counter, self.patience))\n","            if (counter >= self.patience) and (self.best_score > self.at_last_score) and (self.epoch >= self.min_epoch):\n","                self.early_stop = True \n","                self._save_checkpoint()\n","        else:                    \n","            self.best_score = score \n","            self.best_epoch = self.epoch\n","            self.best_model = copy.deepcopy(model).cpu()\n","        \n","        if self.max_epoch <= self.epoch:\n","            self.early_stop = True \n","            self._save_checkpoint()\n","\n","    def _save_checkpoint(self):\n","        if self.model_path is not None and self.best_model is not None:\n","            torch.save(self.best_model.state_dict(), self.model_path.replace('_score','_'+str(self.best_score)))\n","            print('model saved at: ',self.model_path.replace('_score','_'+str(self.best_score)))"],"metadata":{"id":"1EOVwIqucnDs","executionInfo":{"status":"ok","timestamp":1658985131320,"user_tz":-540,"elapsed":500,"user":{"displayName":"조문근","userId":"14792891422128059653"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# CosineAnnealingWarmupRestarts"],"metadata":{"id":"HqvF6nnxc5xq"}},{"cell_type":"code","source":["import math\n","import torch\n","from torch.optim.lr_scheduler import _LRScheduler\n","\n","class CosineAnnealingWarmupRestarts(_LRScheduler):\n","    \"\"\"\n","        optimizer (Optimizer): Wrapped optimizer.\n","        first_cycle_steps (int): First cycle step size.\n","        cycle_mult(float): Cycle steps magnification. Default: -1.\n","        max_lr(float): First cycle's max learning rate. Default: 0.1.\n","        min_lr(float): Min learning rate. Default: 0.001.\n","        warmup_steps(int): Linear warmup step size. Default: 0.\n","        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n","        last_epoch (int): The index of last epoch. Default: -1.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 optimizer: torch.optim.Optimizer,\n","                 first_cycle_steps: int,\n","                 cycle_mult: float = 1.,\n","                 max_lr: float = 0.1,\n","                 min_lr: float = 0.001,\n","                 warmup_steps: int = 0,\n","                 gamma: float = 1.,\n","                 last_epoch: int = -1):\n","        assert warmup_steps < first_cycle_steps\n","\n","        self.first_cycle_steps = first_cycle_steps  # first cycle step size\n","        self.cycle_mult = cycle_mult  # cycle steps magnification\n","        self.base_max_lr = max_lr  # first max learning rate\n","        self.max_lr = max_lr  # max learning rate in the current cycle\n","        self.min_lr = min_lr  # min learning rate\n","        self.warmup_steps = warmup_steps  # warmup step size\n","        self.gamma = gamma  # decrease rate of max learning rate by cycle\n","\n","        self.cur_cycle_steps = first_cycle_steps  # first cycle step size\n","        self.cycle = 0  # cycle count\n","        self.step_in_cycle = last_epoch  # step size of the current cycle\n","\n","        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n","\n","        # set learning rate min_lr\n","        self.init_lr()\n","\n","    def init_lr(self):\n","        self.base_lrs = []\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = self.min_lr\n","            self.base_lrs.append(self.min_lr)\n","\n","    def get_lr(self):\n","        if self.step_in_cycle == -1:\n","            return self.base_lrs\n","        elif self.step_in_cycle < self.warmup_steps:\n","            return [(self.max_lr - base_lr) * self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr + (self.max_lr - base_lr)\n","                    * (1 + math.cos(math.pi * (self.step_in_cycle - self.warmup_steps)\n","                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n","                    for base_lr in self.base_lrs]\n","\n","    def step(self, epoch=None):\n","        if epoch is None:\n","            epoch = self.last_epoch + 1\n","            self.step_in_cycle = self.step_in_cycle + 1\n","            if self.step_in_cycle >= self.cur_cycle_steps:\n","                self.cycle += 1\n","                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n","                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n","        else:\n","            if epoch >= self.first_cycle_steps:\n","                if self.cycle_mult == 1.:\n","                    self.step_in_cycle = epoch % self.first_cycle_steps\n","                    self.cycle = epoch // self.first_cycle_steps\n","                else:\n","                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n","                    self.cycle = n\n","                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n","                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n","            else:\n","                self.cur_cycle_steps = self.first_cycle_steps\n","                self.step_in_cycle = epoch\n","\n","        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n","        self.last_epoch = math.floor(epoch)\n","        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n","            param_group['lr'] = lr\n"],"metadata":{"id":"f4fqQauVc3qU","executionInfo":{"status":"ok","timestamp":1658985131938,"user_tz":-540,"elapsed":619,"user":{"displayName":"조문근","userId":"14792891422128059653"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# train_setup"],"metadata":{"id":"2EATj8mAc-0_"}},{"cell_type":"code","source":["import torch.nn.functional as f\n","import torch.nn as nn\n","import torch\n","import numpy as np\n","from transformers import AutoConfig, AutoModel\n","\n","\n","class _20SampleModel(nn.Module): # 이 새끼 dropout을 안때렸다,,,?\n","    def __init__(self, model_path):\n","        super(_20SampleModel, self).__init__()\n","        \n","        config = AutoConfig.from_pretrained(model_path)\n","        \n","        self.model = AutoModel.from_pretrained(model_path)\n","        self.top = nn.Linear(config.hidden_size+1, 1) # for train_fts\n","\n","    def forward(self, ids, mask, fts, labels=None):\n","        x = self.model(ids, mask)[0]\n","        x = torch.cat((x[:, 0, :], fts), 1)\n","        x = self.top(x)\n","        \n","        if labels is not None:\n","            loss = self.get_loss(x, labels)\n","            return loss, x\n","        else:\n","            return x\n","\n","    def get_loss(self, preds, targets):\n","        loss_fct = nn.L1Loss()\n","        loss = loss_fct(preds, targets)\n","        return loss   "],"metadata":{"id":"T-wotzTNDesT","executionInfo":{"status":"ok","timestamp":1658985131938,"user_tz":-540,"elapsed":9,"user":{"displayName":"조문근","userId":"14792891422128059653"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/ENG/ai4code/src2\n","\n","from transformers import AdamW\n","\n","def train_setup(args):\n","    model = _20SampleModel(model_path=args.model_name_or_path)\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(\n","            nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(\n","            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","    num_train_optimization_steps = args.num_train_steps\n","    optimizer = AdamW(optimizer_grouped_parameters,\n","                      lr=3e-5, correct_bias=False)\n","    scheduler = (\n","        CosineAnnealingWarmupRestarts(\n","            optimizer=optimizer,\n","            first_cycle_steps=num_train_optimization_steps,\n","            cycle_mult=1,\n","            max_lr=args.max_lr,\n","            min_lr=args.min_lr,\n","            warmup_steps=num_train_optimization_steps * 0.2,\n","            gamma=1.,\n","            last_epoch=-1\n","        ))  # Pytorch scheduler\n","\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    return model, optimizer, scheduler, scaler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBMsh1EJc9HL","executionInfo":{"status":"ok","timestamp":1658985134328,"user_tz":-540,"elapsed":2392,"user":{"displayName":"조문근","userId":"14792891422128059653"}},"outputId":"65563309-d5d4-4ab6-a45a-b6043b8af561"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/ENG/ai4code/src2\n"]}]},{"cell_type":"markdown","source":["# AWP"],"metadata":{"id":"Vl0tb_m9dE5d"}},{"cell_type":"code","source":["class AWP:\n","    def __init__(\n","        self,\n","        model,\n","        optimizer,\n","        adv_param=\"weight\",\n","        adv_lr=1,\n","        adv_eps=0.2,\n","        start_epoch=0,\n","        adv_step=1,\n","        scaler=None\n","    ):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.start_epoch = start_epoch\n","        self.adv_step = adv_step\n","        self.backup = {}\n","        self.backup_eps = {}\n","        self.scaler = scaler\n","\n","    def attack_backward(self, *inputs, target, epoch):\n","        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n","            return None\n","\n","        self._save() \n","        for i in range(self.adv_step):\n","            self._attack_step() \n","            with torch.cuda.amp.autocast():\n","                adv_loss, tr_logits = self.model(*inputs, labels=target)\n","                adv_loss = adv_loss.mean()\n","            self.optimizer.zero_grad()\n","            self.scaler.scale(adv_loss).backward()\n","            \n","        self._restore()\n","\n","    def _attack_step(self):\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","                # param.data.clamp_(*self.backup_eps[name])\n","\n","    def _save(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self,):\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}"],"metadata":{"id":"38tIKhgydDO7","executionInfo":{"status":"ok","timestamp":1658985134328,"user_tz":-540,"elapsed":3,"user":{"displayName":"조문근","userId":"14792891422128059653"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# train"],"metadata":{"id":"TxO0sVVAdOmt"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/ENG/ai4code/src2\n","\n","from train import validate, read_data\n","from metrics import kendall_tau\n","\n","import torch, sys, os\n","from tqdm import tqdm\n","import numpy as np\n","\n","\n","def train(model, train_loader, val_loader, optimizer, scheduler, scaler, val_df, df_orders, args):\n","    criterion = torch.nn.L1Loss()\n","    es = EarlyStopping(patience=4,max_epoch=args.epochs)\n","    awp = AWP(model,\n","              optimizer,\n","              adv_lr=1.,\n","              adv_eps=0.2,\n","            #   start_epoch=args.num_train_steps/args.epochs,\n","              start_epoch=0,\n","              scaler=scaler)\n","    preds_score = 0\n","    step = 0\n","\n","    for e in range(args.epoch, 100):\n","        model.train()\n","\n","        tbar = tqdm(train_loader, file=sys.stdout, position=0, leave=True)\n","\n","        loss_list = []\n","        preds = []\n","        labels = []\n","\n","        for idx, data in enumerate(tbar):\n","            step += 1\n","\n","            inputs, target = read_data(data)\n","\n","            with torch.cuda.amp.autocast():\n","                pred, loss = model(*inputs, labels=target)\n","            scaler.scale(loss).backward()\n","\n","            # if preds_score > 0.82:\n","                # input_ids, labels, attention_mask\n","            awp.attack_backward(*inputs, target=target, epoch=step) \n","\n","            if idx % args.accumulation_steps == 0 or idx == len(tbar) - 1:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                scheduler.step()\n","\n","            loss_list.append(loss.detach().cpu().item())\n","            preds.append(pred.detach().cpu().numpy().ravel())\n","            labels.append(target.detach().cpu().numpy().ravel())\n","\n","            avg_loss = np.round(np.mean(loss_list), 4)\n","\n","            tbar.set_description(\n","                f'Epoch {e+1} Loss: {avg_loss} lr: {scheduler.get_lr()}')\n","\n","        y_val, y_pred = validate(model, val_loader)\n","        val_df['pred'] = val_df.groupby(['id', 'cell_type'])['rank'].rank(pct=True)\n","        val_df.loc[val_df['cell_type'] == 'markdown', 'pred'] = y_pred\n","        y_dummy = val_df.sort_values('pred').groupby('id')['cell_id'].apply(list)\n","        preds_score = kendall_tau(df_orders.loc[y_dummy.index], y_dummy)\n","        print(\"Preds score\", preds_score)\n","        \n","        if not os.path.exists(args.output_path):\n","            os.mkdir(args.output_path)\n","            \n","        es(preds_score, model, model_path=args.output_path + f'/model_epoch_{e}_score.bin')\n","        if es.early_stop:\n","            break\n","        # torch.save(model.state_dict(), args.output_path + f'/model_epoch_{e}.bin')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9sUOFK2dMaT","executionInfo":{"status":"ok","timestamp":1658985135161,"user_tz":-540,"elapsed":835,"user":{"displayName":"조문근","userId":"14792891422128059653"}},"outputId":"b2677811-991f-457b-b52c-d90fce8baf42"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/ENG/ai4code/src2\n"]}]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"g7gyv_q-fMsp"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/ENG/ai4code/src2\n","\n","import easydict\n","import gc\n","import pandas as pd\n","\n","from preprocessor import PairwisePreprocessor, _20CodeCellPreprocessor\n","from dataset import _20SampleDataset, PairwiseDataset, _20sample_data_setup, pairwise_data_setup\n","from util import pairwise_debug_setup, _20sample_debug_setup\n","\n","\n","args = {\n","    'model_name_or_path': 'microsoft/graphcodebert-base',\n","\n","    'input_path': '../input/',\n","\n","    'train_path': './data/train.csv',\n","    'train_mark_path': './data/train_mark.csv',\n","    'train_features_path': './data/train_fts.json',\n","\n","    'val_path': \"./data/val.csv\",\n","    'val_mark_path': './data/val_mark.csv',\n","    'val_features_path': './data/val_fts.json',\n","\n","    'output_path': './output-graphcodebert-20sample-debug',\n","\n","    'md_max_len': 64,\n","    'total_max_len': 512,\n","    'batch_size': 16,\n","    'accumulation_steps': 2,\n","    'epoch': 0,\n","    'epochs': 5,\n","    'n_workers': 8,\n","    'debug': True,\n","    'load_train': False,\n","    'max_lr': 3e-5,\n","    'min_lr': .3e-6,\n","    'kfold': True\n","}\n","\n","args = easydict.EasyDict(args)\n","\n","preprocessor = _20CodeCellPreprocessor(**vars(args))\n","train_df, val_df, train_df_mark, val_df_mark, train_fts, val_fts = preprocessor.run()\n","\n","print('before debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n","\n","kfolds = []\n","if args.debug:\n","    for i in range(5):\n","        fold = _20sample_debug_setup(train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts)\n","        kfolds.append(fold)\n","    \n","train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[0]\n","print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n","\n","train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[1]\n","print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n","\n","train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[2]\n","print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n","\n","train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[3]\n","print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n","\n","train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[4]\n","print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n","\n","df_orders = pd.read_csv(args.input_path + 'train_orders.csv',\n","                        index_col='id',\n","                        squeeze=True).str.split()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbZTEgZEfIEc","outputId":"0b21c13a-4c22-493d-f650-fa48dd3ae843","executionInfo":{"status":"ok","timestamp":1658985453769,"user_tz":-540,"elapsed":318609,"user":{"displayName":"조문근","userId":"14792891422128059653"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/ENG/ai4code/src2\n","train_df, val_df are already exits\n","train_fts, val_fts are already exists\n","before debug (5740832, 8) (629814, 8) (1950118, 8) (629814, 8) 125292 13964\n","after debug (579267, 8) (60771, 8) (196927, 8) (60771, 8) 12529 1396\n","after debug (572979, 8) (66500, 8) (194946, 8) (66500, 8) 12529 1396\n","after debug (571492, 8) (62541, 8) (195165, 8) (62541, 8) 12529 1396\n","after debug (573870, 8) (60704, 8) (194794, 8) (60704, 8) 12529 1396\n","after debug (573752, 8) (64702, 8) (195745, 8) (64702, 8) 12529 1396\n"]}]},{"cell_type":"code","source":["for i in range(5):\n","    train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[i]\n","\n","    train_loader, val_loader = _20sample_data_setup(train_df_mark, val_df_mark, train_fts, val_fts, args)\n","\n","    del train_df, train_df_mark, train_fts\n","    gc.collect()\n","\n","    args.num_train_steps = args.epochs * len(train_loader) / args.accumulation_steps\n","\n","    model, optimizer, scheduler, scaler = train_setup(args)\n","    model.cuda()\n","\n","    train(model, train_loader, val_loader, optimizer, scheduler, scaler, val_df, df_orders, args)\n","\n","    del model, optimizer, scheduler, scaler, val_fts, train_loader, val_loader\n","    gc.collect()"],"metadata":{"id":"aPTlFWx9zUZv","colab":{"base_uri":"https://localhost:8080/","height":592},"executionInfo":{"status":"error","timestamp":1658985470259,"user_tz":-540,"elapsed":16495,"user":{"displayName":"조문근","userId":"14792891422128059653"}},"outputId":"1d265b90-f326-4527-c1b5-9615c1b5ab64"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["  0%|          | 0/12307 [00:02<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-83d149728af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_orders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_fts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-3a3d8a35c561>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, scaler, val_df, df_orders, args)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# if preds_score > 0.82:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"5HNW4uA0Fnb4","executionInfo":{"status":"aborted","timestamp":1658985470259,"user_tz":-540,"elapsed":5,"user":{"displayName":"조문근","userId":"14792891422128059653"}}},"execution_count":null,"outputs":[]}]}