{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "from preprocessor import PairwisePreprocessor, _20CodeCellPreprocessor\n",
    "from dataset import _20SampleDataset, PairwiseDataset, _20sample_data_setup, pairwise_data_setup\n",
    "from train import pairwise_train_setup, train_setup\n",
    "from util import pairwise_debug_setup, _20sample_debug_setup\n",
    "from metrics import kendall_tau\n",
    "from train import train\n",
    "\n",
    "args = {\n",
    "    'model_name_or_path': 'microsoft/graphcodebert-base',\n",
    "\n",
    "    'input_path': '../input/',\n",
    "\n",
    "    'train_path': './data/train.csv',\n",
    "    'train_mark_path': './data/train_mark.csv',\n",
    "    'train_features_path': './data/train_fts.json',\n",
    "\n",
    "    'val_path': \"./data/val.csv\",\n",
    "    'val_mark_path': './data/val_mark.csv',\n",
    "    'val_features_path': './data/val_fts.json',\n",
    "\n",
    "    'output_path': './output-graphcodebert-20sample-debug',\n",
    "\n",
    "    'md_max_len': 64,\n",
    "    'total_max_len': 512,\n",
    "    'batch_size': 32,\n",
    "    'accumulation_steps': 1,\n",
    "    'epoch': 0,\n",
    "    'epochs': 5,\n",
    "    'n_workers': 8,\n",
    "    'debug': True,\n",
    "    'load_train': False,\n",
    "    'max_lr': 3e-5,\n",
    "    'min_lr': .3e-6,\n",
    "    'kfold': True\n",
    "}\n",
    "\n",
    "args = easydict.EasyDict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df, val_df are already exits\n",
      "train_fts, val_fts are already exists\n",
      "before debug (5740832, 8) (629814, 8) (1950118, 8) (629814, 8) 125292 13964\n",
      "after debug (566977, 8) (60759, 8) (192487, 8) (60759, 8) 12529 1396\n",
      "after debug (570218, 8) (63777, 8) (192522, 8) (63777, 8) 12529 1396\n",
      "after debug (575337, 8) (62287, 8) (193953, 8) (62287, 8) 12529 1396\n",
      "after debug (566148, 8) (62488, 8) (193033, 8) (62488, 8) 12529 1396\n",
      "after debug (563686, 8) (63828, 8) (189789, 8) (63828, 8) 12529 1396\n"
     ]
    }
   ],
   "source": [
    "preprocessor = _20CodeCellPreprocessor(**vars(args))\n",
    "train_df, val_df, train_df_mark, val_df_mark, train_fts, val_fts = preprocessor.run()\n",
    "\n",
    "print('before debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n",
    "\n",
    "kfolds = []\n",
    "if args.debug:\n",
    "    for i in range(5):\n",
    "        fold = _20sample_debug_setup(train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts)\n",
    "        kfolds.append(fold)\n",
    "    \n",
    "train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[0]\n",
    "print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n",
    "\n",
    "train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[1]\n",
    "print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n",
    "\n",
    "train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[2]\n",
    "print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n",
    "\n",
    "train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[3]\n",
    "print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n",
    "\n",
    "train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[4]\n",
    "print('after debug', train_df.shape, val_df.shape, train_df_mark.shape, val_df.shape, len(train_fts), len(val_fts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ansrm\\AppData\\Local\\Temp\\ipykernel_28884\\677948621.py:1: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  df_orders = pd.read_csv(args.input_path + 'train_orders.csv',\n",
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.4063 lr: [1.9950124688279302e-08, 1.9950124688279302e-08]:   0%|          | 1/6015 [00:19<31:48:10, 19.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.1822 lr: [2.526315789473684e-05, 2.526315789473684e-05]: 100%|██████████| 6015/6015 [37:32<00:00,  2.67it/s]  \n",
      "100%|██████████| 653/653 [01:32<00:00,  7.09it/s]\n",
      "Preds score 0.779829136959287\n",
      "Epoch 2 Loss: 0.1305 lr: [1.894736842105263e-05, 1.894736842105263e-05]: 100%|██████████| 6015/6015 [37:15<00:00,  2.69it/s]  \n",
      "100%|██████████| 653/653 [01:28<00:00,  7.34it/s]\n",
      "Preds score 0.8066387884338179\n",
      "Epoch 3 Loss: 0.1131 lr: [1.263157894736842e-05, 1.263157894736842e-05]: 100%|██████████| 6015/6015 [37:02<00:00,  2.71it/s]  \n",
      "100%|██████████| 653/653 [01:27<00:00,  7.43it/s]\n",
      "Preds score 0.8065701257641509\n",
      "Epoch 4 Loss: 0.1008 lr: [6.31578947368421e-06, 6.31578947368421e-06]: 100%|██████████| 6015/6015 [37:00<00:00,  2.71it/s]    \n",
      "100%|██████████| 653/653 [01:28<00:00,  7.42it/s]\n",
      "Preds score 0.8115441565615875\n",
      "Epoch 5 Loss: 0.092 lr: [0.0, 0.0]: 100%|██████████| 6015/6015 [36:59<00:00,  2.71it/s]                                       \n",
      "100%|██████████| 653/653 [01:27<00:00,  7.43it/s]\n",
      "Preds score 0.8119026292006283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.4688 lr: [1.9946808510638297e-08, 1.9946808510638297e-08]:   0%|          | 1/6016 [00:15<25:09:55, 15.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2457 lr: [2.526315789473684e-05, 2.526315789473684e-05]: 100%|██████████| 6016/6016 [37:09<00:00,  2.70it/s]  \n",
      "100%|██████████| 702/702 [01:34<00:00,  7.43it/s]\n",
      "Preds score 0.612228226146861\n",
      "Epoch 2 Loss: 0.2574 lr: [1.894736842105263e-05, 1.894736842105263e-05]: 100%|██████████| 6016/6016 [38:29<00:00,  2.61it/s]  \n",
      "100%|██████████| 702/702 [01:34<00:00,  7.46it/s]\n",
      "Preds score 0.6128817611899084\n",
      "Epoch 3 Loss: 0.2568 lr: [1.263157894736842e-05, 1.263157894736842e-05]: 100%|██████████| 6016/6016 [37:19<00:00,  2.69it/s]  \n",
      "100%|██████████| 702/702 [01:34<00:00,  7.39it/s]\n",
      "Preds score 0.6153550669339356\n",
      "Epoch 4 Loss: 0.2564 lr: [6.31578947368421e-06, 6.31578947368421e-06]: 100%|██████████| 6016/6016 [37:20<00:00,  2.69it/s]    \n",
      "100%|██████████| 702/702 [01:34<00:00,  7.43it/s]\n",
      "Preds score 0.6155473296645173\n",
      "Epoch 5 Loss: 0.2561 lr: [0.0, 0.0]: 100%|██████████| 6016/6016 [37:18<00:00,  2.69it/s]                                      \n",
      "100%|██████████| 702/702 [01:34<00:00,  7.44it/s]\n",
      "Preds score 0.6166834995456507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3389 lr: [1.979871308364956e-08, 1.979871308364956e-08]:   0%|          | 1/6061 [00:15<25:32:09, 15.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.1788 lr: [2.526315789473684e-05, 2.526315789473684e-05]: 100%|██████████| 6061/6061 [37:31<00:00,  2.69it/s]  \n",
      "100%|██████████| 678/678 [01:30<00:00,  7.46it/s]\n",
      "Preds score 0.7987488574016076\n",
      "Epoch 2 Loss: 0.1294 lr: [1.894736842105263e-05, 1.894736842105263e-05]: 100%|██████████| 6061/6061 [37:40<00:00,  2.68it/s]  \n",
      "100%|██████████| 678/678 [01:31<00:00,  7.40it/s]\n",
      "Preds score 0.8097194923588006\n",
      "Epoch 3 Loss: 0.1124 lr: [1.263157894736842e-05, 1.263157894736842e-05]: 100%|██████████| 6061/6061 [37:32<00:00,  2.69it/s]  \n",
      "100%|██████████| 678/678 [01:31<00:00,  7.42it/s]\n",
      "Preds score 0.8160483147439671\n",
      "Epoch 4 Loss: 0.1004 lr: [6.31578947368421e-06, 6.31578947368421e-06]: 100%|██████████| 6061/6061 [37:32<00:00,  2.69it/s]    \n",
      "100%|██████████| 678/678 [01:31<00:00,  7.44it/s]\n",
      "Preds score 0.8149994273801862\n",
      "Epoch 5 Loss: 0.0917 lr: [0.0, 0.0]: 100%|██████████| 6061/6061 [37:19<00:00,  2.71it/s]                                      \n",
      "100%|██████████| 678/678 [01:31<00:00,  7.43it/s]\n",
      "Preds score 0.8164413305571485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7305 lr: [1.9893899204244032e-08, 1.9893899204244032e-08]:   0%|          | 1/6032 [00:14<24:58:56, 14.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.1874 lr: [2.526315789473684e-05, 2.526315789473684e-05]: 100%|██████████| 6032/6032 [37:08<00:00,  2.71it/s]  \n",
      "100%|██████████| 674/674 [01:30<00:00,  7.46it/s]\n",
      "Preds score 0.7787443863345459\n",
      "Epoch 2 Loss: 0.132 lr: [1.894736842105263e-05, 1.894736842105263e-05]: 100%|██████████| 6032/6032 [37:20<00:00,  2.69it/s]   \n",
      "100%|██████████| 674/674 [01:31<00:00,  7.37it/s]\n",
      "Preds score 0.7990120828513242\n",
      "Epoch 3 Loss: 0.1147 lr: [1.263157894736842e-05, 1.263157894736842e-05]: 100%|██████████| 6032/6032 [37:29<00:00,  2.68it/s]  \n",
      "100%|██████████| 674/674 [01:30<00:00,  7.42it/s]\n",
      "Preds score 0.8031986346861846\n",
      "Epoch 4 Loss: 0.1022 lr: [6.31578947368421e-06, 6.31578947368421e-06]: 100%|██████████| 6032/6032 [37:10<00:00,  2.70it/s]    \n",
      "100%|██████████| 674/674 [01:34<00:00,  7.14it/s]\n",
      "Preds score 0.8048780058676824\n",
      "Epoch 5 Loss: 0.0935 lr: [0.0, 0.0]: 100%|██████████| 6032/6032 [37:33<00:00,  2.68it/s]                                      \n",
      "100%|██████████| 674/674 [01:31<00:00,  7.36it/s]\n",
      "Preds score 0.8053979159610639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3822 lr: [2.0236087689713324e-08, 2.0236087689713324e-08]:   0%|          | 1/5930 [00:15<24:57:04, 15.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ansrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.1775 lr: [2.526315789473684e-05, 2.526315789473684e-05]: 100%|██████████| 5930/5930 [36:46<00:00,  2.69it/s]  \n",
      "100%|██████████| 672/672 [01:30<00:00,  7.40it/s]\n",
      "Preds score 0.7898881606158608\n",
      "Epoch 2 Loss: 0.1289 lr: [1.894736842105263e-05, 1.894736842105263e-05]: 100%|██████████| 5930/5930 [36:44<00:00,  2.69it/s]  \n",
      "100%|██████████| 672/672 [01:29<00:00,  7.47it/s]\n",
      "Preds score 0.8076668483923037\n",
      "Epoch 3 Loss: 0.1116 lr: [1.263157894736842e-05, 1.263157894736842e-05]: 100%|██████████| 5930/5930 [36:30<00:00,  2.71it/s]  \n",
      "100%|██████████| 672/672 [01:30<00:00,  7.47it/s]\n",
      "Preds score 0.8196607404374079\n",
      "Epoch 4 Loss: 0.0992 lr: [6.31578947368421e-06, 6.31578947368421e-06]: 100%|██████████| 5930/5930 [36:48<00:00,  2.69it/s]    \n",
      "100%|██████████| 672/672 [01:30<00:00,  7.46it/s]\n",
      "Preds score 0.8160648752301378\n",
      "Epoch 5 Loss: 0.0899 lr: [0.0, 0.0]: 100%|██████████| 5930/5930 [36:34<00:00,  2.70it/s]                                      \n",
      "100%|██████████| 672/672 [01:29<00:00,  7.47it/s]\n",
      "Preds score 0.8176773819009489\n"
     ]
    }
   ],
   "source": [
    "df_orders = pd.read_csv(args.input_path + 'train_orders.csv',\n",
    "                        index_col='id',\n",
    "                        squeeze=True).str.split()\n",
    "\n",
    "for i in range(5):\n",
    "    train_df, train_df_mark, train_fts, val_df, val_df_mark, val_fts = kfolds[i]\n",
    "\n",
    "    train_loader, val_loader = _20sample_data_setup(train_df_mark, val_df_mark, train_fts, val_fts, args)\n",
    "\n",
    "    del train_df, train_df_mark, train_fts\n",
    "    gc.collect()\n",
    "\n",
    "    args.num_train_steps = args.epochs * len(train_loader) / args.accumulation_steps\n",
    "\n",
    "    model, optimizer, scheduler, scaler = train_setup(args)\n",
    "    model.cuda()\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, scaler, val_df, df_orders, args)\n",
    "\n",
    "    del model, optimizer, scheduler, scaler, val_fts, train_loader, val_loader\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df134e92ff705b7dadd626b38eae65b2c3b3d6a1f5a931787b11d90c02adb825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
