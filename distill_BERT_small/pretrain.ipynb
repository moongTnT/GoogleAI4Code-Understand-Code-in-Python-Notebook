{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pretrain.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNNZ8DWJF6482y4eSU7rpeM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qHlI7pJI3D6","executionInfo":{"status":"ok","timestamp":1657960416332,"user_tz":-540,"elapsed":19041,"user":{"displayName":"조문근","userId":"14792891422128059653"}},"outputId":"5bc09092-151b-4a61-cbe3-347185dbedc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Sat Jul 16 08:33:35 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","!nvidia-smi"]},{"cell_type":"code","source":["!pip install -q transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xSZPbFpMJM_x","executionInfo":{"status":"ok","timestamp":1657960468863,"user_tz":-540,"elapsed":9466,"user":{"displayName":"조문근","userId":"14792891422128059653"}},"outputId":"6c19d00d-1c58-47f5-940e-d556c5cd56ed"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.4 MB 13.4 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 77.9 MB/s \n","\u001b[K     |████████████████████████████████| 101 kB 11.9 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 71.2 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","%cd /content/drive/MyDrive/NLP/ENG/ai4code/src-pairwise\n","from Pretrainer import Pretrainer\n","\n","pt = Pretrainer(None)\n","pt.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"uKunDT0MI8Bi","executionInfo":{"status":"ok","timestamp":1657973495278,"user_tz":-540,"elapsed":12814884,"user":{"displayName":"조문근","userId":"14792891422128059653"}},"outputId":"63ff011d-a6f3-4af8-8cd5-ce5e9e384760"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/ENG/ai4code/src-pairwise\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n","Model config BertConfig {\n","  \"_name_or_path\": \"prajjwal1/bert-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 512,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 2048,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 8,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/68be80309844e53b628e9d479926a991d0adf337752bb941f0188887240313b8.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n","Model config BertConfig {\n","  \"_name_or_path\": \"prajjwal1/bert-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 512,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 2048,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 8,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n","Model config BertConfig {\n","  \"_name_or_path\": \"prajjwal1/bert-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 512,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 2048,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 8,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n","loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n","Model config BertConfig {\n","  \"_name_or_path\": \"prajjwal1/bert-small\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 512,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 2048,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 8,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/prajjwal1/bert-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/facfdb1638fdec899406e0efd5c2c43ae4bbafcb45dd15f68df1f2378e3e70fb.59547972ec02ba39d4ea413c843f1638e8f90e118a4334ae5d626bf7524ac597\n","Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertForMaskedLM were initialized from the model checkpoint at prajjwal1/bert-small.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Creating features from dataset file at ./data-all/text.txt\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 531928\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 83120\n"]},{"output_type":"stream","name":"stdout","text":["No. of lines:  531928\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='83120' max='83120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [83120/83120 3:31:25, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>4.329900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>4.001500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.848400</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>3.756400</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>3.690900</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>3.640800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>3.603900</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>3.563400</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>3.518800</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>3.490000</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>3.458200</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>3.436400</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>3.433700</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>3.409000</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>3.367600</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>3.368700</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>3.334900</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>3.294000</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>3.309600</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>3.294100</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>3.272400</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>3.260800</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>3.265300</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>3.227100</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>3.199700</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>3.206100</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>3.198300</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>3.183200</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>3.192200</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>3.180600</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>3.155000</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>3.166800</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>3.155500</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>3.128900</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>3.123900</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>3.136600</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>3.106100</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>3.108000</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>3.103800</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>3.086500</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>3.092900</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>3.101600</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>3.066900</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>3.074100</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>3.069700</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>3.051100</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>3.046200</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>3.062000</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>3.046600</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>3.043700</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>3.016700</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>3.015000</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>3.005000</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>3.019000</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>3.006500</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>2.992600</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>2.995300</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>3.000100</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>2.991000</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>2.994900</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>2.981000</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>2.985700</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>2.983600</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>2.990900</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>2.959300</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>2.973300</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>2.954500</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>2.941000</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>2.942800</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>2.949300</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>2.937600</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>2.938000</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>2.942100</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>2.925000</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>2.900400</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>2.915300</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>2.926600</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>2.918400</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>2.924900</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>2.902300</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>2.941600</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>2.935700</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>2.932000</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>2.888400</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>2.895400</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>2.897800</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>2.900700</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>2.887200</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>2.891200</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>2.882200</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>2.872100</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>2.881400</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>2.878300</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>2.880000</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>2.873800</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>2.866300</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>2.869600</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>2.886900</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>2.879400</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>2.864200</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>2.839800</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>2.834500</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>2.850400</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>2.845700</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>2.854000</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>2.866700</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>2.844000</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>2.844000</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>2.833600</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>2.855500</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>2.842600</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>2.826600</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>2.836300</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>2.837700</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>2.847000</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>2.839500</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>2.812200</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>2.821000</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>2.810800</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>2.825100</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>2.830400</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>2.821000</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>2.822800</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>2.809100</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>2.827000</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>2.791900</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>2.809600</td>\n","    </tr>\n","    <tr>\n","      <td>64000</td>\n","      <td>2.812100</td>\n","    </tr>\n","    <tr>\n","      <td>64500</td>\n","      <td>2.800600</td>\n","    </tr>\n","    <tr>\n","      <td>65000</td>\n","      <td>2.813100</td>\n","    </tr>\n","    <tr>\n","      <td>65500</td>\n","      <td>2.787400</td>\n","    </tr>\n","    <tr>\n","      <td>66000</td>\n","      <td>2.796400</td>\n","    </tr>\n","    <tr>\n","      <td>66500</td>\n","      <td>2.828000</td>\n","    </tr>\n","    <tr>\n","      <td>67000</td>\n","      <td>2.786000</td>\n","    </tr>\n","    <tr>\n","      <td>67500</td>\n","      <td>2.796600</td>\n","    </tr>\n","    <tr>\n","      <td>68000</td>\n","      <td>2.822400</td>\n","    </tr>\n","    <tr>\n","      <td>68500</td>\n","      <td>2.767900</td>\n","    </tr>\n","    <tr>\n","      <td>69000</td>\n","      <td>2.797900</td>\n","    </tr>\n","    <tr>\n","      <td>69500</td>\n","      <td>2.799400</td>\n","    </tr>\n","    <tr>\n","      <td>70000</td>\n","      <td>2.794500</td>\n","    </tr>\n","    <tr>\n","      <td>70500</td>\n","      <td>2.791800</td>\n","    </tr>\n","    <tr>\n","      <td>71000</td>\n","      <td>2.780500</td>\n","    </tr>\n","    <tr>\n","      <td>71500</td>\n","      <td>2.797800</td>\n","    </tr>\n","    <tr>\n","      <td>72000</td>\n","      <td>2.782000</td>\n","    </tr>\n","    <tr>\n","      <td>72500</td>\n","      <td>2.792100</td>\n","    </tr>\n","    <tr>\n","      <td>73000</td>\n","      <td>2.776000</td>\n","    </tr>\n","    <tr>\n","      <td>73500</td>\n","      <td>2.778900</td>\n","    </tr>\n","    <tr>\n","      <td>74000</td>\n","      <td>2.774800</td>\n","    </tr>\n","    <tr>\n","      <td>74500</td>\n","      <td>2.787700</td>\n","    </tr>\n","    <tr>\n","      <td>75000</td>\n","      <td>2.774300</td>\n","    </tr>\n","    <tr>\n","      <td>75500</td>\n","      <td>2.777200</td>\n","    </tr>\n","    <tr>\n","      <td>76000</td>\n","      <td>2.793200</td>\n","    </tr>\n","    <tr>\n","      <td>76500</td>\n","      <td>2.776600</td>\n","    </tr>\n","    <tr>\n","      <td>77000</td>\n","      <td>2.783800</td>\n","    </tr>\n","    <tr>\n","      <td>77500</td>\n","      <td>2.772600</td>\n","    </tr>\n","    <tr>\n","      <td>78000</td>\n","      <td>2.793300</td>\n","    </tr>\n","    <tr>\n","      <td>78500</td>\n","      <td>2.766800</td>\n","    </tr>\n","    <tr>\n","      <td>79000</td>\n","      <td>2.783600</td>\n","    </tr>\n","    <tr>\n","      <td>79500</td>\n","      <td>2.770300</td>\n","    </tr>\n","    <tr>\n","      <td>80000</td>\n","      <td>2.787000</td>\n","    </tr>\n","    <tr>\n","      <td>80500</td>\n","      <td>2.769700</td>\n","    </tr>\n","    <tr>\n","      <td>81000</td>\n","      <td>2.772700</td>\n","    </tr>\n","    <tr>\n","      <td>81500</td>\n","      <td>2.768400</td>\n","    </tr>\n","    <tr>\n","      <td>82000</td>\n","      <td>2.774100</td>\n","    </tr>\n","    <tr>\n","      <td>82500</td>\n","      <td>2.780900</td>\n","    </tr>\n","    <tr>\n","      <td>83000</td>\n","      <td>2.763900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./data-all/pretrained/checkpoint-10000\n","Configuration saved in ./data-all/pretrained/checkpoint-10000/config.json\n","Model weights saved in ./data-all/pretrained/checkpoint-10000/pytorch_model.bin\n","Saving model checkpoint to ./data-all/pretrained/checkpoint-20000\n","Configuration saved in ./data-all/pretrained/checkpoint-20000/config.json\n","Model weights saved in ./data-all/pretrained/checkpoint-20000/pytorch_model.bin\n","Saving model checkpoint to ./data-all/pretrained/checkpoint-30000\n","Configuration saved in ./data-all/pretrained/checkpoint-30000/config.json\n","Model weights saved in ./data-all/pretrained/checkpoint-30000/pytorch_model.bin\n","Saving model checkpoint to ./data-all/pretrained/checkpoint-40000\n","Configuration saved in ./data-all/pretrained/checkpoint-40000/config.json\n","Model weights saved in ./data-all/pretrained/checkpoint-40000/pytorch_model.bin\n","Saving model checkpoint to ./data-all/pretrained/checkpoint-50000\n","Configuration saved in ./data-all/pretrained/checkpoint-50000/config.json\n","Model weights saved in ./data-all/pretrained/checkpoint-50000/pytorch_model.bin\n","Saving model checkpoint to ./data-all/pretrained/checkpoint-60000\n","Configuration saved in ./data-all/pretrained/checkpoint-60000/config.json\n","Model weights saved in ./data-all/pretrained/checkpoint-60000/pytorch_model.bin\n","Saving model checkpoint to ./data-all/pretrained/checkpoint-70000\n","Configuration saved in ./data-all/pretrained/checkpoint-70000/config.json\n","Model weights saved in ./data-all/pretrained/checkpoint-70000/pytorch_model.bin\n","Saving model checkpoint to ./data-all/pretrained/checkpoint-80000\n","Configuration saved in ./data-all/pretrained/checkpoint-80000/config.json\n","Model weights saved in ./data-all/pretrained/checkpoint-80000/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./data-all/pretrained/\n","Configuration saved in ./data-all/pretrained/config.json\n","Model weights saved in ./data-all/pretrained/pytorch_model.bin\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"0FZtHLTh0BT2"},"execution_count":null,"outputs":[]}]}